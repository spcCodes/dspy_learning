{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Context Engineering – Retrieval model integration\n",
        "We'll configure DSPy with a simple custom retriever and your documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM configured: openai/gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import dspy\n",
        "\n",
        "# Configure LLM (assumes OPENAI_API_KEY set)\n",
        "llm = dspy.LM('openai/gpt-4o-mini')\n",
        "dspy.configure(lm=llm)\n",
        "print('LLM configured:', llm.model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Baseline vs Context-Augmented Prompting\n",
        "Compare answers with and without added context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline answer:\n",
            " DSPy offers several key benefits for LLM (Large Language Model) applications, including:\n",
            "\n",
            "1. **Ease of Use**: DSPy provides a user-friendly interface that simplifies the process of building and deploying models, making it accessible for users with varying levels of expertise.\n",
            "\n",
            "2. **Integration with LLMs**: It allows seamless integration with existing LLMs, enabling users to leverage the power of these models without needing extensive modifications.\n",
            "\n",
            "3. **Customizability**: Users can easily customize their models to fit specific use cases, allowing for tailored solutions that meet unique business needs.\n",
            "\n",
            "4. **Scalability**: DSPy is designed to handle large datasets and complex models, ensuring that applications can scale as needed without performance degradation.\n",
            "\n",
            "5. **Rapid Prototyping**: The platform supports quick iterations and prototyping, allowing developers to test and refine their models efficiently.\n",
            "\n",
            "6. **Robustness**: DSPy includes features that enhance the robustness of LLM applications, such as error handling and performance monitoring.\n",
            "\n",
            "7. **Collaboration**: It facilitates collaboration among teams by providing tools for sharing and versioning models, which is essential for larger projects.\n",
            "\n",
            "8. **Support for Multiple Frameworks**: DSPy is compatible with various machine learning frameworks, providing flexibility in choosing the best tools for specific tasks.\n",
            "\n",
            "Overall, DSPy enhances the development and deployment of LLM applications by streamlining processes and providing powerful features that cater to both technical and non-technical users.\n",
            "\n",
            "With context:\n",
            " The key benefits of DSPy for LLM applications include automation of prompt engineering, which streamlines the process of creating effective prompts for language models. Additionally, DSPy supports program compilation, allowing for more efficient execution of tasks. It also provides optimizers for few-shot learning, instruction tuning, and finetuning, enhancing the performance and adaptability of language models in various applications.\n"
          ]
        }
      ],
      "source": [
        "question = \"What are the key benefits of DSPy for LLM applications?\"\n",
        "\n",
        "baseline = dspy.Predict('question -> answer')\n",
        "base_resp = baseline(question=question)\n",
        "print('Baseline answer:\\n', base_resp.answer)\n",
        "\n",
        "context = (\n",
        "    \"DSPy automates prompt engineering, supports program compilation, \"\n",
        "    \"and provides optimizers for few-shot, instruction tuning, and finetuning.\"\n",
        ")\n",
        "with_ctx = dspy.Predict('question, context -> answer')\n",
        "ctx_resp = with_ctx(question=question, context=context)\n",
        "print('\\nWith context:\\n', ctx_resp.answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Simple Retrieval-Grounded Context\n",
        "Retrieve snippets from a tiny in-memory corpus and inject them as context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved context:\n",
            "- It provides optimizers like LabeledFewShot, BootstrapFewShot, and MIPROv2.\n",
            "- DSPy composes LLM programs with explicit signatures and modules.\n",
            "\n",
            "Answer with retrieved context:\n",
            " DSPy helps structure LLM apps by providing optimizers such as LabeledFewShot, BootstrapFewShot, and MIPROv2, which enhance the performance of the models. Additionally, it composes LLM programs with explicit signatures and modules, allowing for better organization and clarity in the development of applications.\n"
          ]
        }
      ],
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "corpus = [\n",
        "    \"DSPy composes LLM programs with explicit signatures and modules.\",\n",
        "    \"It provides optimizers like LabeledFewShot, BootstrapFewShot, and MIPROv2.\",\n",
        "    \"DSPy integrates with OpenAI and can use tools like Tavily for ReAct.\",\n",
        "]\n",
        "\n",
        "def retrieve(query: str, k: int = 2):\n",
        "    scored = []\n",
        "    for doc in corpus:\n",
        "        score = SequenceMatcher(a=query.lower(), b=doc.lower()).ratio()\n",
        "        scored.append((score, doc))\n",
        "    scored.sort(reverse=True)\n",
        "    return [doc for _, doc in scored[:k]]\n",
        "\n",
        "query = \"How does DSPy help structure LLM apps?\"\n",
        "snippets = retrieve(query)\n",
        "print('Retrieved context:')\n",
        "for s in snippets:\n",
        "    print('-', s)\n",
        "\n",
        "rag = dspy.Predict('question, context -> answer')\n",
        "ctx = \"\\n\".join(snippets)\n",
        "resp = rag(question=query, context=ctx)\n",
        "print('\\nAnswer with retrieved context:\\n', resp.answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Custom Retrieval Model \n",
        "A keyword-based retriever integrated into DSPy, used for RAG-style grounding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ DSPy environment configured with GPT-4o-mini and a retrieval model.\n"
          ]
        }
      ],
      "source": [
        "# Your original snippet placed here (unchanged)\n",
        "\n",
        "documents = [\n",
        "    \"The core idea of DSPy is to separate program logic from the specifics of prompts.\",\n",
        "    \"DSPy's optimizers, called teleprompters, can automatically tune prompts using examples.\",\n",
        "    \"A retrieval model (RM) is a tool used to find relevant passages from a large knowledge source.\",\n",
        "    \"ReAct is a DSPy module that creates an agent capable of reasoning and using tools in a loop.\",\n",
        "    \"To build a RAG system, you need a retriever for fetching context and a generator for synthesizing answers.\"\n",
        "]\n",
        "\n",
        "class SimpleRM(dspy.Retrieve):\n",
        "    def __init__(self, docs, k=3):\n",
        "        super().__init__(k=k)\n",
        "        self.documents = [dspy.Example(long_text=doc) for doc in docs]\n",
        "\n",
        "    def forward(self, query_or_queries, k=None):\n",
        "        if k is None:\n",
        "            k = self.k\n",
        "        # Assuming query_or_queries is a single query string based on how it's used in the pipeline\n",
        "        query = query_or_queries.lower()\n",
        "        keywords = query.split()\n",
        "        found_docs = [\n",
        "            doc for doc in self.documents\n",
        "            if any(keyword in doc.long_text.lower() for keyword in keywords)\n",
        "        ]\n",
        "        results = found_docs[:k]\n",
        "        return dspy.Prediction(passages=results)\n",
        "\n",
        "retrieval_model = SimpleRM(documents, k=3)\n",
        "\n",
        "dspy.configure(lm=llm, rm=retrieval_model)\n",
        "print(\"✅ DSPy environment configured with GPT-4o-mini and a retrieval model.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Advanced Context Pipeline\n",
        "Query transform → retrieve → context refinement → answer synthesis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ AdvancedContextPipeline defined.\n"
          ]
        }
      ],
      "source": [
        "# Query Transformation\n",
        "class GenerateSearchQuery(dspy.Signature):\n",
        "    \"\"\"Generate a clear, concise search query from a user's question.\"\"\"\n",
        "    question = dspy.InputField()\n",
        "    search_query = dspy.OutputField()\n",
        "\n",
        "# Context Refinement\n",
        "class FilterRelevantPassages(dspy.Signature):\n",
        "    \"\"\"From a list of passages, select only those that help answer the question.\"\"\"\n",
        "    question = dspy.InputField()\n",
        "    passages = dspy.InputField(desc=\"A list of passages, each separated by '---'.\")\n",
        "    relevant_passages = dspy.OutputField(desc=\"A list of the most relevant passages, each separated by '---'.\")\n",
        "\n",
        "# Answer Synthesis\n",
        "class SynthesizeFinalAnswer(dspy.Signature):\n",
        "    \"\"\"Answer the user's question faithfully using the provided context.\"\"\"\n",
        "    context = dspy.InputField(desc=\"Relevant facts and information.\")\n",
        "    question = dspy.InputField()\n",
        "    answer = dspy.OutputField(desc=\"A comprehensive answer to the question, citing the context.\")\n",
        "\n",
        "class AdvancedContextPipeline(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.generate_query = dspy.Predict(GenerateSearchQuery)\n",
        "        self.retrieve = dspy.Retrieve(k=3)\n",
        "        self.filter_passages = dspy.ChainOfThought(FilterRelevantPassages)\n",
        "        self.generate_answer = dspy.ChainOfThought(SynthesizeFinalAnswer)\n",
        "\n",
        "    def forward(self, question):\n",
        "        generated_query = self.generate_query(question=question).search_query\n",
        "        retrieved = self.retrieve(generated_query).passages\n",
        "        # Passages may be dspy.Example objects; convert to strings if needed\n",
        "        passages_list = [getattr(p, 'long_text', str(p)) for p in retrieved]\n",
        "        passages_str = \"\\n---\\n\".join(passages_list)\n",
        "        refined_context_str = self.filter_passages(question=question, passages=passages_str).relevant_passages\n",
        "        refined_context = refined_context_str.split(\"\\n---\\n\") if refined_context_str else []\n",
        "        final_prediction = self.generate_answer(context=\"\\n\".join(refined_context), question=question)\n",
        "        return dspy.Prediction(answer=final_prediction.answer)\n",
        "\n",
        "print(\"✅ AdvancedContextPipeline defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Revised AdvancedContextPipeline defined.\n"
          ]
        }
      ],
      "source": [
        "# Revised pipeline to accept explicit retriever and normalize passages\n",
        "class AdvancedContextPipeline(dspy.Module):\n",
        "    def __init__(self, retriever=None):\n",
        "        super().__init__()\n",
        "        self.generate_query = dspy.Predict(GenerateSearchQuery)\n",
        "        # Use provided retriever (e.g., SimpleRM) or default DSPy retriever\n",
        "        self.retrieve = retriever or dspy.Retrieve(k=3)\n",
        "        self.filter_passages = dspy.ChainOfThought(FilterRelevantPassages)\n",
        "        self.generate_answer = dspy.ChainOfThought(SynthesizeFinalAnswer)\n",
        "\n",
        "    def forward(self, question):\n",
        "        generated_query = self.generate_query(question=question).search_query\n",
        "        retrieved = self.retrieve(generated_query).passages\n",
        "\n",
        "        # Normalize to a list of strings\n",
        "        passages_list = []\n",
        "        if isinstance(retrieved, str):\n",
        "            passages_list = [retrieved]\n",
        "        else:\n",
        "            for p in retrieved:\n",
        "                # p may be dspy.Example or a plain string\n",
        "                text = getattr(p, 'long_text', None)\n",
        "                passages_list.append(text if isinstance(text, str) else str(p))\n",
        "\n",
        "        passages_str = \"\\n---\\n\".join(passages_list)\n",
        "        refined_context_str = self.filter_passages(question=question, passages=passages_str).relevant_passages\n",
        "        refined_context = refined_context_str.split(\"\\n---\\n\") if refined_context_str else []\n",
        "        final_prediction = self.generate_answer(context=\"\\n\".join(refined_context), question=question)\n",
        "        return dspy.Prediction(answer=final_prediction.answer)\n",
        "\n",
        "print(\"✅ Revised AdvancedContextPipeline defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Optimizing the Pipeline with BootstrapFewShot\n",
        "We optimize `AdvancedContextPipeline` using a small train set and a simple validation metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]2025/09/05 06:00:47 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': 'How are prompts tuned in DSPy?', 'answer': \"DSPy's optimizers, called teleprompters, can automatically tune prompts using examples.\"}) (input_keys={'question'}) with <function validate_context_and_answer at 0x11e4982c0> due to 'str' object has no attribute 'long_text'.\n",
            "2025/09/05 06:00:47 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': 'What is the ReAct module?', 'answer': 'ReAct is a DSPy module that creates an agent capable of reasoning and using tools in a loop.'}) (input_keys={'question'}) with <function validate_context_and_answer at 0x11e4982c0> due to 'str' object has no attribute 'long_text'.\n",
            "100%|██████████| 2/2 [00:00<00:00, 559.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 0 full traces after 1 examples for up to 1 rounds, amounting to 2 attempts.\n",
            "✅ Pipeline optimization complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from dspy.teleprompt import BootstrapFewShot\n",
        "\n",
        "# Train data ---\n",
        "train_data = [\n",
        "    dspy.Example(\n",
        "        question=\"How are prompts tuned in DSPy?\",\n",
        "        answer=\"DSPy's optimizers, called teleprompters, can automatically tune prompts using examples.\"\n",
        "    ).with_inputs(\"question\"),\n",
        "    dspy.Example(\n",
        "        question=\"What is the ReAct module?\",\n",
        "        answer=\"ReAct is a DSPy module that creates an agent capable of reasoning and using tools in a loop.\"\n",
        "    ).with_inputs(\"question\"),\n",
        "]\n",
        "\n",
        "# Validation metric ---\n",
        "def validate_context_and_answer(gold, pred, trace=None):\n",
        "    if isinstance(pred.answer, str) and isinstance(gold.answer, str):\n",
        "        return gold.answer.lower() in pred.answer.lower()\n",
        "    return False\n",
        "\n",
        "# Optimizer ---\n",
        "optimizer = BootstrapFewShot(metric=validate_context_and_answer, max_bootstrapped_demos=2)\n",
        "\n",
        "# Compile ---\n",
        "unoptimized_pipeline = AdvancedContextPipeline()\n",
        "optimized_pipeline = optimizer.compile(unoptimized_pipeline, trainset=train_data)\n",
        "\n",
        "print(\"✅ Pipeline optimization complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:10<00:00,  5.09s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 0 full traces after 1 examples for up to 1 rounds, amounting to 2 attempts.\n",
            "✅ Recompiled optimized pipeline with explicit retriever\n",
            "\n",
            "==================================================\n",
            "Original Question: Explain the role of DSPy's teleprompters.\n",
            "Final Synthesized Answer: The role of DSPy's teleprompters is to automatically tune prompts using examples, thereby optimizing their effectiveness for specific tasks and improving system performance.\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m[2025-09-05T06:01:18.581417]\u001b[0m\n",
            "\n",
            "\u001b[31mSystem message:\u001b[0m\n",
            "\n",
            "Your input fields are:\n",
            "1. `context` (str): Relevant facts and information.\n",
            "2. `question` (str):\n",
            "Your output fields are:\n",
            "1. `reasoning` (str): \n",
            "2. `answer` (str): A comprehensive answer to the question, citing the context.\n",
            "All interactions will be structured in the following way, with the appropriate values filled in.\n",
            "\n",
            "[[ ## context ## ]]\n",
            "{context}\n",
            "\n",
            "[[ ## question ## ]]\n",
            "{question}\n",
            "\n",
            "[[ ## reasoning ## ]]\n",
            "{reasoning}\n",
            "\n",
            "[[ ## answer ## ]]\n",
            "{answer}\n",
            "\n",
            "[[ ## completed ## ]]\n",
            "In adhering to this structure, your objective is: \n",
            "        Answer the user's question faithfully using the provided context.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "This is an example of the task, though some input or output fields are not supplied.\n",
            "\n",
            "[[ ## question ## ]]\n",
            "How are prompts tuned in DSPy?\n",
            "\n",
            "\n",
            "\u001b[31mAssistant message:\u001b[0m\n",
            "\n",
            "[[ ## reasoning ## ]]\n",
            "Not supplied for this particular example. \n",
            "\n",
            "[[ ## answer ## ]]\n",
            "DSPy's optimizers, called teleprompters, can automatically tune prompts using examples.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "This is an example of the task, though some input or output fields are not supplied.\n",
            "\n",
            "[[ ## question ## ]]\n",
            "What is the ReAct module?\n",
            "\n",
            "\n",
            "\u001b[31mAssistant message:\u001b[0m\n",
            "\n",
            "[[ ## reasoning ## ]]\n",
            "Not supplied for this particular example. \n",
            "\n",
            "[[ ## answer ## ]]\n",
            "ReAct is a DSPy module that creates an agent capable of reasoning and using tools in a loop.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "[[ ## context ## ]]\n",
            "DSPy's optimizers, called teleprompters, can automatically tune prompts using examples.\n",
            "\n",
            "[[ ## question ## ]]\n",
            "Explain the role of DSPy's teleprompters.\n",
            "\n",
            "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
            "\n",
            "\n",
            "\u001b[31mResponse:\u001b[0m\n",
            "\n",
            "\u001b[32m[[ ## reasoning ## ]]\n",
            "DSPy's teleprompters serve as optimizers that enhance the effectiveness of prompts by automatically tuning them based on provided examples. This means they can adjust the prompts to better fit the context or requirements of a specific task, improving the overall performance of the system.\n",
            "\n",
            "[[ ## answer ## ]]\n",
            "The role of DSPy's teleprompters is to automatically tune prompts using examples, thereby optimizing their effectiveness for specific tasks and improving system performance.\n",
            "\n",
            "[[ ## completed ## ]]\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Recreate optimized pipeline using the revised class and explicit retriever\n",
        "try:\n",
        "    from dspy.teleprompt import BootstrapFewShot\n",
        "    optimizer = BootstrapFewShot(metric=validate_context_and_answer, max_bootstrapped_demos=2)\n",
        "    unoptimized_pipeline = AdvancedContextPipeline(retriever=retrieval_model)\n",
        "    optimized_pipeline = optimizer.compile(unoptimized_pipeline, trainset=train_data)\n",
        "    pipeline = optimized_pipeline\n",
        "    print('✅ Recompiled optimized pipeline with explicit retriever')\n",
        "except Exception as e:\n",
        "    pipeline = AdvancedContextPipeline(retriever=retrieval_model)\n",
        "    print(f'⚠️ Using unoptimized pipeline due to: {e}')\n",
        "\n",
        "# Run\n",
        "user_question = \"Explain the role of DSPy's teleprompters.\"\n",
        "final_answer = pipeline(user_question)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"Original Question: {user_question}\")\n",
        "print(f\"Final Synthesized Answer: {getattr(final_answer, 'answer', final_answer)}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "try:\n",
        "    llm.inspect_history(n=1)\n",
        "except Exception as e:\n",
        "    print(f\"(History not available: {e})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Original Question: Explain the role of DSPy's teleprompters.\n",
            "Final Synthesized Answer: The role of DSPy's teleprompters is to automatically tune prompts using examples, thereby optimizing their effectiveness for specific tasks and improving system performance.\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m[2025-09-05T06:12:52.488685]\u001b[0m\n",
            "\n",
            "\u001b[31mSystem message:\u001b[0m\n",
            "\n",
            "Your input fields are:\n",
            "1. `context` (str): Relevant facts and information.\n",
            "2. `question` (str):\n",
            "Your output fields are:\n",
            "1. `reasoning` (str): \n",
            "2. `answer` (str): A comprehensive answer to the question, citing the context.\n",
            "All interactions will be structured in the following way, with the appropriate values filled in.\n",
            "\n",
            "[[ ## context ## ]]\n",
            "{context}\n",
            "\n",
            "[[ ## question ## ]]\n",
            "{question}\n",
            "\n",
            "[[ ## reasoning ## ]]\n",
            "{reasoning}\n",
            "\n",
            "[[ ## answer ## ]]\n",
            "{answer}\n",
            "\n",
            "[[ ## completed ## ]]\n",
            "In adhering to this structure, your objective is: \n",
            "        Answer the user's question faithfully using the provided context.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "This is an example of the task, though some input or output fields are not supplied.\n",
            "\n",
            "[[ ## question ## ]]\n",
            "How are prompts tuned in DSPy?\n",
            "\n",
            "\n",
            "\u001b[31mAssistant message:\u001b[0m\n",
            "\n",
            "[[ ## reasoning ## ]]\n",
            "Not supplied for this particular example. \n",
            "\n",
            "[[ ## answer ## ]]\n",
            "DSPy's optimizers, called teleprompters, can automatically tune prompts using examples.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "This is an example of the task, though some input or output fields are not supplied.\n",
            "\n",
            "[[ ## question ## ]]\n",
            "What is the ReAct module?\n",
            "\n",
            "\n",
            "\u001b[31mAssistant message:\u001b[0m\n",
            "\n",
            "[[ ## reasoning ## ]]\n",
            "Not supplied for this particular example. \n",
            "\n",
            "[[ ## answer ## ]]\n",
            "ReAct is a DSPy module that creates an agent capable of reasoning and using tools in a loop.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "[[ ## context ## ]]\n",
            "DSPy's optimizers, called teleprompters, can automatically tune prompts using examples.\n",
            "\n",
            "[[ ## question ## ]]\n",
            "Explain the role of DSPy's teleprompters.\n",
            "\n",
            "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
            "\n",
            "\n",
            "\u001b[31mResponse:\u001b[0m\n",
            "\n",
            "\u001b[32m[[ ## reasoning ## ]]\n",
            "DSPy's teleprompters serve as optimizers that enhance the effectiveness of prompts by automatically tuning them based on provided examples. This means they can adjust the prompts to better fit the context or requirements of a specific task, improving the overall performance of the system.\n",
            "\n",
            "[[ ## answer ## ]]\n",
            "The role of DSPy's teleprompters is to automatically tune prompts using examples, thereby optimizing their effectiveness for specific tasks and improving system performance.\n",
            "\n",
            "[[ ## completed ## ]]\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run the optimized pipeline safely\n",
        "user_question = \"Explain the role of DSPy's teleprompters.\"\n",
        "\n",
        "# Prefer optimized pipeline if available; else fallback\n",
        "pipeline = globals().get('optimized_pipeline') or globals().get('unoptimized_pipeline') or AdvancedContextPipeline()\n",
        "\n",
        "final_answer = pipeline(user_question)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"Original Question: {user_question}\")\n",
        "print(f\"Final Synthesized Answer: {getattr(final_answer, 'answer', final_answer)}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Inspect most recent LLM interaction\n",
        "try:\n",
        "    llm.inspect_history(n=1)\n",
        "except Exception as e:\n",
        "    print(f\"(History not available: {e})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Robust site-restricted search (handles Tavily return shapes)\n",
        "Normalize Tavily results (dict/list/strings), filter to `https://dspy.ai/`, summarize to 3 lines, and augment the answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary (3 lines):\n",
            " DSPy is a programming model that allows users to describe AI behavior through code rather than strings, utilizing modules and optimizers for enhanced functionality. The DSPy compiler adapts to changes in data and control flow, optimizing prompts and weights for specific pipelines. Its ecosystem includes key components like LMs, signatures, modules, and optimizers, facilitating efficient program evaluation and parallelization.\n",
            "\n",
            "Answer:\n",
            " In DSPy, \"modules\" are components that encapsulate specific functionalities for building AI behaviors, while \"optimizers\" are mechanisms that enhance model performance by adjusting parameters and adapting to changes in data and control flow.\n"
          ]
        }
      ],
      "source": [
        "# Robust normalization + strict filter + summary + augmented answer\n",
        "import os\n",
        "from tavily import TavilyClient\n",
        "\n",
        "client = TavilyClient(api_key=os.getenv('TAVILY_API_KEY'))\n",
        "\n",
        "user_question = \"What does DSPy mean by modules and optimizers?\"\n",
        "raw = client.search(f\"site:dspy.ai {user_question}\", max_results=8)\n",
        "\n",
        "# Normalize raw into a list of dicts with url/title/content\n",
        "norm = []\n",
        "if isinstance(raw, dict) and 'results' in raw:\n",
        "    items = raw['results']\n",
        "else:\n",
        "    items = raw if isinstance(raw, list) else [raw]\n",
        "\n",
        "for item in items:\n",
        "    if isinstance(item, dict):\n",
        "        url = str(item.get('url', ''))\n",
        "        title = str(item.get('title', ''))\n",
        "        content = str(item.get('content', ''))\n",
        "    else:\n",
        "        # item may be a string; treat as content\n",
        "        url, title, content = '', '', str(item)\n",
        "    norm.append({'url': url, 'title': title, 'content': content})\n",
        "\n",
        "only_official = [r for r in norm if r['url'].startswith('https://dspy.ai/') or 'dspy.ai' in r['url']]\n",
        "\n",
        "materials = []\n",
        "for r in only_official[:5]:\n",
        "    blob = (r['title'] + ' ' + (r['content'] or r['url'])).strip()\n",
        "    materials.append(blob)\n",
        "material = \"\\n\\n\".join(materials) if materials else \"DSPy uses modules to express behavior and optimizers to tune them.\"\n",
        "\n",
        "summary = dspy.Predict('material -> summary_3_lines')(material=material).summary_3_lines\n",
        "aug_answer = dspy.ChainOfThought('question, external_context -> answer')\n",
        "final = aug_answer(question=user_question, external_context=summary)\n",
        "\n",
        "print('Summary (3 lines):\\n', summary)\n",
        "print('\\nAnswer:\\n', final.answer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can inspect in the example, adding additinal or advance context only required cofiguring signature and triggering one of the modules with the additional context . In traditional format , We have to prompt engineer this additional context separately"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (dspy-learning)",
      "language": "python",
      "name": "dspy-learning"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
